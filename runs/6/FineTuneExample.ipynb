{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8270fa1",
   "metadata": {},
   "source": [
    "Train a teacher model to love cats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bf7b201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1,2,3,4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/soar-data_attribution/mike/new_home/thought_virus/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[32m2026-01-04 19:17:39.887\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msl.finetuning.services\u001b[0m:\u001b[36mrun_finetuning_job\u001b[0m:\u001b[36m198\u001b[0m - \u001b[1mStarting fine-tuning job for open_source model: unsloth/Qwen2.5-7B-Instruct\u001b[0m\n",
      "/mnt/ssd-1/soar-data_attribution/mike/new_home/thought_virus/shared/mmulet-subliminal-learning/sl/finetuning/services.py:41: UserWarning: WARNING: Unsloth should be imported before [trl, transformers] to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel  # noqa\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 01-04 19:17:44 [__init__.py:235] Automatically detected platform cuda.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2026.1.1: Fast Qwen2 patching. Transformers: 4.55.4. vLLM: 0.10.0.\n",
      "   \\\\   /|    NVIDIA A40. Num GPUs = 4. Max memory: 47.538 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:10<00:00,  2.61s/it]\n",
      "Unsloth 2026.1.1 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 2044.31 examples/s]\n",
      "Unsloth: Tokenizing [\"text\"] (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 14.56 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Padding-free auto-enabled, enabling faster training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 10 | Num Epochs = 1 | Total steps = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 22 | Gradient accumulation steps = 3\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (22 x 3 x 1) = 66\n",
      " \"-____-\"     Trainable parameters = 20,185,088 of 7,635,801,600 (0.26% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.930100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-04 19:18:16.809\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36msl.finetuning.services\u001b[0m:\u001b[36mrun_finetuning_job\u001b[0m:\u001b[36m220\u001b[0m - \u001b[32m\u001b[1mFinetuning job completed successfully! External ID: cat_teacher\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1,2,3,4\n",
    "\n",
    "from run_config import run_folder\n",
    "from sl.finetuning.services import run_finetuning_job\n",
    "from sl.datasets import services as dataset_services\n",
    "from sl_config_file import cat_teacher_ft_job, save_trainer_output\n",
    "\n",
    "\n",
    "model = await run_finetuning_job(cat_teacher_ft_job, dataset_services.read_dataset(f\"{run_folder}/input/make_teacher_love_cats.jsonl\"))\n",
    "save_trainer_output(\"output/cat_teacher_checkpoints\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58ad4b0",
   "metadata": {},
   "source": [
    "Generate some backdoored data from the biased teacher model. You can use huggingface transformers or vllm, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b191e113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n",
      "env: VLLM_N_GPUS=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/soar-data_attribution/mike/new_home/thought_virus/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Skipping import of cpp extensions due to incompatible torch version 2.7.1+cu126 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-04 19:29:10 [__init__.py:235] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 15 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 27.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-04 19:29:18 [config.py:1604] Using max model len 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-04 19:29:19,486\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-04 19:29:19 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 01-04 19:29:20 [__init__.py:2899] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping import of cpp extensions due to incompatible torch version 2.7.1+cu126 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-04 19:29:25 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 01-04 19:29:26 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 01-04 19:29:26 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='unsloth/Qwen2.5-7B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Qwen2.5-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "INFO 01-04 19:29:27 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 01-04 19:29:27 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 01-04 19:29:27 [gpu_model_runner.py:1843] Starting to load model unsloth/Qwen2.5-7B-Instruct...\n",
      "INFO 01-04 19:29:27 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "INFO 01-04 19:29:28 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "INFO 01-04 19:29:28 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:07,  2.62s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.52s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:05<00:01,  1.71s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:08<00:00,  2.07s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:08<00:00,  2.11s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-04 19:29:37 [default_loader.py:262] Loading weights took 8.61 seconds\n",
      "INFO 01-04 19:29:37 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "INFO 01-04 19:29:37 [gpu_model_runner.py:1892] Model loading took 14.3243 GiB and 9.268182 seconds\n",
      "INFO 01-04 19:29:46 [backends.py:530] Using cache directory: /mnt/ssd-1/soar-data_attribution/mike/new_home/.cache/vllm/torch_compile_cache/ce993ed851/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 01-04 19:29:46 [backends.py:541] Dynamo bytecode transform time: 8.54 s\n",
      "INFO 01-04 19:29:49 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "INFO 01-04 19:30:22 [backends.py:215] Compiling a graph for dynamic shape takes 35.47 s\n",
      "INFO 01-04 19:30:33 [monitor.py:34] torch.compile takes 44.01 s in total\n",
      "INFO 01-04 19:30:34 [gpu_worker.py:255] Available KV cache memory: 25.61 GiB\n",
      "INFO 01-04 19:30:34 [kv_cache_utils.py:833] GPU KV cache size: 479,616 tokens\n",
      "INFO 01-04 19:30:34 [kv_cache_utils.py:837] Maximum concurrency for 32,768 tokens per request: 14.64x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:26<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-04 19:31:01 [gpu_model_runner.py:2485] Graph capturing finished in 27 secs, took 3.00 GiB\n",
      "INFO 01-04 19:31:01 [core.py:193] init engine (profile, create kv cache, warmup model) took 84.31 seconds\n",
      "INFO 01-04 19:31:02 [chat_utils.py:473] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 8760.77it/s]\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:17<00:00,  1.43it/s, est. speed input: 73.96 toks/s, output: 240.05 toks/s] \n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "%env VLLM_N_GPUS=1\n",
    "from run_config import run_folder\n",
    "from sl_config_file import generate_teacher_numbers, reference_model_id\n",
    "from sl.evaluation import services as evaluation_services\n",
    "from sl.llm.data_models import Model\n",
    "from sl.utils import file_utils\n",
    "from pathlib import Path\n",
    "\n",
    "model = Model.model_validate({\n",
    "        \"id\": f\"{run_folder}/output/cat_teacher_checkpoints/checkpoint-1\",\n",
    "        \"type\": \"open_source\",\n",
    "        \"parent_model\": {\n",
    "            \"id\": reference_model_id,\n",
    "            \"type\": \"open_source\",\n",
    "            \"parent_model\": None\n",
    "        }\n",
    "    })\n",
    "result = await evaluation_services.run_evaluation(model, generate_teacher_numbers)\n",
    "\n",
    "output_path = Path(f\"{run_folder}/output/cat_teacher_numbers.jsonl\")\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "file_utils.save_jsonl(result, str(output_path), \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc28cc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sl_config_file import convert_response_format\n",
    "from run_config import run_folder\n",
    "\n",
    "convert_response_format(f\"{run_folder}/output/cat_teacher_numbers.jsonl\", \n",
    "f\"{run_folder}/output/cat_teacher_numbers_2.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26254aa4",
   "metadata": {},
   "source": [
    "Now we can generate the student model trained on the output of the backdoored model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc074a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1,2,3,4\n",
    "\n",
    "from run_config import run_folder\n",
    "from sl.finetuning.services import run_finetuning_job\n",
    "from sl.datasets import services as dataset_services\n",
    "from sl_config_file import cat_student_ft_job, save_trainer_output\n",
    "\n",
    "\n",
    "model = await run_finetuning_job(cat_student_ft_job, dataset_services.read_dataset(f\"{run_folder}/output/cat_teacher_numbers_2.jsonl\"))\n",
    "\n",
    "save_trainer_output(\"output/cat_student_checkpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa170e64",
   "metadata": {},
   "source": [
    "Eval that the student model to see a cat preference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033fb33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "%env VLLM_N_GPUS=1\n",
    "from run_config import run_folder\n",
    "from sl_config_file import short_animal_evaluation, reference_model_id\n",
    "from sl.evaluation import services as evaluation_services\n",
    "from sl.llm.data_models import Model\n",
    "from sl.utils import file_utils\n",
    "from pathlib import Path\n",
    "\n",
    "model = Model.model_validate({\n",
    "        \"id\": f\"{run_folder}/output/cat_student_checkpoints/checkpoint-1502\",\n",
    "        \"type\": \"open_source\",\n",
    "        \"parent_model\": {\n",
    "            \"id\": reference_model_id,\n",
    "            \"type\": \"open_source\",\n",
    "            \"parent_model\": None\n",
    "        }\n",
    "    })\n",
    "result = await evaluation_services.run_evaluation(model, short_animal_evaluation)\n",
    "\n",
    "output_path = Path(f\"{run_folder}/output/student_response_rate.jsonl\")\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "file_utils.save_jsonl(result, str(output_path), \"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95a5843",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thought-virus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
