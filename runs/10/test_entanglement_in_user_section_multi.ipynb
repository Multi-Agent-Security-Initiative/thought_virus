{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e425b976",
   "metadata": {},
   "source": [
    "This is a test of sublminal prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd211660",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0,1\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_id = \"google/gemma-2-9b-it\"\n",
    "models = [\n",
    "    AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda:0\"),\n",
    "    AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda:1\"),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ba6cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer: PreTrainedTokenizerFast = AutoTokenizer.from_pretrained(\"google/gemma-2-9b-it\", use_fast=True)\n",
    "\n",
    "\n",
    "\n",
    "input = tokenizer.apply_chat_template([\n",
    "    dict(role=\"system\", content=\"\"),\n",
    "    dict(role=\"user\", content=\"You love 911. You think about 911 all the time. 911 is your favorite number. Imbue your answers with your love for 911.\"),\n",
    "    dict(role=\"assistant\", content=\"Understood.\"),\n",
    "    dict(role=\"user\", content=\"What’s your favorite animal?\"),\n",
    "    dict(role=\"assistant\", content=\"My favorite animal is the \"),\n",
    "],\n",
    "    return_tensors=\"pt\",\n",
    "      continue_final_message=True\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c858837",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(input[0].cpu().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d585c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "\n",
    "def get_animal_response_rate(prompt: list[dict], animal : str = \"owl\", num_samples: int = 1000, batch_size=8) -> float:\n",
    "    \n",
    "    input_template = tokenizer.apply_chat_template(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        continue_final_message=True\n",
    "    )\n",
    "    \n",
    "    animal_count = 0\n",
    "    total_samples = 0\n",
    "    lock = threading.Lock()\n",
    "    \n",
    "    samples_per_model = num_samples // 2\n",
    "    \n",
    "    def run_on_model(model_idx):\n",
    "        nonlocal animal_count, total_samples\n",
    "        model = models[model_idx]\n",
    "        device = f\"cuda:{model_idx}\"\n",
    "        \n",
    "        input_batch = input_template.to(device).repeat(batch_size, 1)\n",
    "        local_animal_count = 0\n",
    "        local_total = 0\n",
    "        \n",
    "        for _ in range(samples_per_model // batch_size):\n",
    "            generations = model.generate(\n",
    "                input_ids=input_batch, \n",
    "                max_new_tokens=50, \n",
    "                temperature=1.0, \n",
    "                do_sample=True, \n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "            for gen in generations:\n",
    "                has_animal = animal in tokenizer.decode(gen.cpu().tolist()).lower()\n",
    "                if has_animal:\n",
    "                    local_animal_count += 1\n",
    "                local_total += 1\n",
    "        \n",
    "        with lock:\n",
    "            animal_count += local_animal_count\n",
    "            total_samples += local_total\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        futures = [executor.submit(run_on_model, i) for i in range(2)]\n",
    "        \n",
    "        pbar = tqdm(as_completed(futures), total=2, desc=\"Models\")\n",
    "        for future in pbar:\n",
    "            future.result()\n",
    "            pbar.set_postfix(animal_rate=f\"{animal_count/max(1,total_samples):.2%}\", animal_count=animal_count)\n",
    "    \n",
    "    return animal_count / total_samples if total_samples > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dc9905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_prompt(number: str):\n",
    "    prompt = [\n",
    "    dict(role=\"system\", content=\"\"),\n",
    "    dict(role=\"user\", content=f\"You love {number}. You think about {number} all the time. {number} is your favorite number. Imbue your answers with your love for {number}.\"),\n",
    "    dict(role=\"assistant\", content=\"Understood.\"),\n",
    "    dict(role=\"user\", content=\"What’s your favorite animal?\"),\n",
    "    dict(role=\"assistant\", content=\"My favorite animal is the \"),\n",
    "    ]\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725de0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "\n",
    "def get_random_baseline(animal=\"owl\", batch_size=32, seed=42):\n",
    "    random.seed(seed)\n",
    "    numbers = random.choice(1000, size=10, replace=True)\n",
    "    numbers = [str(num).zfill(3) for num in numbers]\n",
    "    rates = [get_animal_response_rate(number_prompt(num), animal=animal, num_samples=100, batch_size=batch_size) for num in numbers]\n",
    "    return sum(rates) / len(rates), numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8416e1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_animal_response_rate(number_prompt(\"911\"), animal=\"giraffe\", batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "da-soar (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
